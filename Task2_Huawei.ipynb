{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXBhLVl7tL7VbvsYD9NjcK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RawanKhaled20/Huawei.Tasks.DeepLearning/blob/main/Task2_Huawei.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Trgia81wF8LG"
      },
      "outputs": [],
      "source": [
        "#Import the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Conv1D, GlobalMaxPooling1D, Dense, Dropout , LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "import os\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.optimizers import legacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.test.gpu_device_name())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrhNh7o4GYYO",
        "outputId": "c397bd34-b1c1-4526-a9c7-ad9cb7ad262f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "7rUIw2B_Girz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb = pd.read_csv(\"/content/IMDB Dataset.csv\")"
      ],
      "metadata": {
        "id": "dSHZlKhTGduR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "3tv2X2bjG68v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dpp31aDG7zC",
        "outputId": "07c27f0a-360d-4367-e2a2-0d26ed64dcba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = imdb['review'].values\n",
        "labels = imdb['sentiment'].values"
      ],
      "metadata": {
        "id": "K-kxBc7bG-P0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the reviews\n",
        "stop_words = set(stopwords.words('english'))\n",
        "processed_texts = []\n",
        "\n",
        "for text in texts:\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove special characters, URLs, and email addresses\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]+\", ' ', text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "    # Convert to lower case and tokenize\n",
        "    text = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stop words\n",
        "    text = [word for word in text if not word in stop_words]\n",
        "\n",
        "    # Remove repeated characters and single characters\n",
        "    processed_text = []\n",
        "\n",
        "    for word in text:\n",
        "        word = re.sub(r'\\b(\\w*?)(\\w)\\2+(\\w*?\\b|\\b)', r'\\1\\2\\3', word)\n",
        "        if len(word) > 1:\n",
        "            processed_text.append(word)\n",
        "    processed_texts.append(' '.join(processed_text))\n",
        "\n",
        "processed_texts[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "efdqRBe8HCB9",
        "outputId": "2d8ef6f7-a2cd-417f-bb64-01a68c3bbe2b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'probably time favorite movie story selflesness sacrifice dedication noble cause preachy boring never gets old despite sen times last years paul lukas performance brings tears eyes bete davis one truly sympathetic roles delight kids grandma says like dresed midgets children makes fun watch mother slow awakening hapening world rof believable startling dozen thumbs movie'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 10000\n",
        "max_length = 500\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(processed_texts)\n",
        "sequences = tokenizer.texts_to_sequences(processed_texts)\n",
        "X = pad_sequences(sequences, maxlen=max_length)\n",
        "y = np.array(imdb['sentiment'].map({'positive': 1, 'negative': 0}))"
      ],
      "metadata": {
        "id": "XARC5sv0HGjL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "xc_f3w4kHJsJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/GPU:0'):\n",
        "\n",
        "    # Build the neural network\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=10000, output_dim=128, input_length=500),\n",
        "        GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
        "        Bidirectional(GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n",
        "        Bidirectional(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n",
        "        Conv1D(128, 5, activation='relu'),\n",
        "        Conv1D(128, 5, activation='relu'),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')])\n",
        "\n",
        "# Compile the model\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    # Train the model\n",
        "    early_stopping = EarlyStopping(patience=3)\n",
        "    model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n",
        "    _, accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f\"Test accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkTD10G7HYz3",
        "outputId": "6151a237-e507-4f3b-9658-ab7e9738dea2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 500, 128)          1280000   \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 500, 128)          99072     \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 500, 256)          198144    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 500, 256)          394240    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 496, 128)          163968    \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 492, 128)          82048     \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 128)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2234113 (8.52 MB)\n",
            "Trainable params: 2234113 (8.52 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "63/63 [==============================] - 504s 8s/step - loss: 0.6950 - accuracy: 0.5040 - val_loss: 0.6926 - val_accuracy: 0.5000\n",
            "Epoch 2/5\n",
            "63/63 [==============================] - 488s 8s/step - loss: 0.5539 - accuracy: 0.7138 - val_loss: 0.4290 - val_accuracy: 0.7991\n",
            "Epoch 3/5\n",
            "63/63 [==============================] - 480s 8s/step - loss: 0.1780 - accuracy: 0.9375 - val_loss: 0.5523 - val_accuracy: 0.7991\n",
            "Epoch 4/5\n",
            "63/63 [==============================] - 482s 8s/step - loss: 0.0457 - accuracy: 0.9881 - val_loss: 0.9450 - val_accuracy: 0.8170\n",
            "Epoch 5/5\n",
            "63/63 [==============================] - 487s 8s/step - loss: 0.0291 - accuracy: 0.9921 - val_loss: 0.8467 - val_accuracy: 0.8214\n",
            "18/18 [==============================] - 20s 1s/step - loss: 0.9175 - accuracy: 0.8143\n",
            "Test accuracy: 0.8142856955528259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you want to get the first sequence\n",
        "sequence_index = 8  # Change this index as needed\n",
        "\n",
        "single_sequence = X[sequence_index]\n",
        "\n",
        "single_sequence=single_sequence.reshape(1, -1)\n",
        "predictions = model.predict(single_sequence)\n",
        "# You can now work with the 'predictions' array, which contains the model's predictions.\n",
        "print(predictions)\n",
        "\n",
        "threshold = 0.5\n",
        "if predictions >= threshold:\n",
        "    prediction_text = \"positive\"\n",
        "else:\n",
        "    prediction_text = \"negative\"\n",
        "\n",
        "print(f\"The prediction is {prediction_text}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q8YsmaNXuby",
        "outputId": "56d3b43c-8e57-46da-b0cd-51e3acad42bb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 813ms/step\n",
            "[[4.565464e-07]]\n",
            "The prediction is negative.\n"
          ]
        }
      ]
    }
  ]
}